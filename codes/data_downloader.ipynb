{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import skmob\n",
    "from skmob.preprocessing import compression\n",
    "from skmob.tessellation import tilers\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from shapely import wkt\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "from collections import Counter\n",
    "\n",
    "import folium\n",
    "\n",
    "OUT_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gowalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://snap.stanford.edu/data/loc-gowalla_totalCheckins.txt.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(url, sep='\\t', header=0,\n",
    "     names=['user', 'check-in_time', 'latitude', 'longitude', 'location id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output must be a csv with the following data:\n",
    "\n",
    "# user_id, checkin_id, - , -, latitude, longitude, -, time \n",
    "\n",
    "# with time matching the foramt %a %b %d %H:%M:%S %z %Y\n",
    "\n",
    "# by simply following the aformentioned constraints, we have a csv that can be parsed by DeepMove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['none-1'] = -1\n",
    "df['none-2'] = -1\n",
    "df['none-3'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['user', 'location id', 'none-1', 'none-2', 'latitude', 'longitude', 'none-3', 'check-in_time']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['check-in_time'] = pd.to_datetime(df['check-in_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['check-in_time'] = df['check-in_time'].dt.strftime('%a %b %d %H:%M:%S %z %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['user','check-in_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(OUT_PATH + 'gowalla.txt', header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Porto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1710670it [08:37, 3306.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from csv import reader\n",
    "from ast import literal_eval\n",
    "\n",
    "sdada = pd.read_csv('data/original_data/taxi_porto/train.csv')\n",
    "subset_ids = set(list(sdada.TAXI_ID.unique())[:3])\n",
    "\n",
    "df_elems = []\n",
    "\n",
    "with open('data/original_data/taxi_porto/train.csv', 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    header = next(csv_reader)\n",
    "    for row in tqdm(csv_reader):\n",
    "        measurements = literal_eval(row[8])\n",
    "        for measument in measurements:\n",
    "            if len(measument) > 1:\n",
    "                # remove very far away points \n",
    "                if measument[0] < 7.5 and measument[1] > 40 and measument[1] < 42 and int(row[4]) in subset_ids:\n",
    "                    df_elems.append([row[0],row[4],row[5],measument[0], measument[1]])\n",
    "                 \n",
    "porto_df = pd.DataFrame(df_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_df = pd.DataFrame(df_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_df.rename(columns={0:'trip_id', 1:'user', 2:'time', 3:'longitude', 4:'latitude'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_df['time'] = pd.to_datetime(porto_df['time'],unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_tdf = skmob.TrajDataFrame(porto_df, latitude='latitude', longitude='longitude', datetime='time', user_id='trip_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_tdf = porto_tdf.sort_by_uid_and_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_tdf = compression.compress(porto_tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 250\n",
    "tess_porto = tilers.tiler.get(\"squared\", base_shape='Porto, Portugal', meters=tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_points = gpd.GeoDataFrame(porto_tdf, geometry=gpd.points_from_xy(porto_tdf.lng, porto_tdf.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/mobility_baselines/venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: CRS mismatch between the CRS of left geometries and the CRS of right geometries.\n",
      "Use `to_crs()` to reproject one of the input geometries to match the CRS of the other.\n",
      "\n",
      "Left CRS: None\n",
      "Right CRS: EPSG:4326\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "mapped_porto_tdf = gpd.sjoin(porto_points, tess_porto, how=\"inner\", op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_porto_tdf = skmob.TrajDataFrame(mapped_porto_tdf, latitude='lat', longitude='lng', datetime='datetime', user_id='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_porto_tdf = mapped_porto_tdf.sort_by_uid_and_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        self.venues = {}\n",
    "        self.words_original = []\n",
    "        self.words_lens = []\n",
    "        self.dictionary = dict()\n",
    "        self.words_dict = None\n",
    "        self.data_filter = {}\n",
    "        self.user_filter3 = None\n",
    "        self.uid_list = {}\n",
    "        self.vid_list = {'unk': [0, -1]}\n",
    "        self.vid_list_lookup = {}\n",
    "        self.vid_lookup = {}\n",
    "        self.pid_loc_lat = {}\n",
    "        self.data_neural = {}\n",
    "        \n",
    "        self.train_split = 0.7\n",
    "        self.validation_split = 0.1\n",
    "    \n",
    "    def load_trajectories(self, df):\n",
    "        for i, row in df.iterrows():\n",
    "            uid = row['user']\n",
    "            pid = row['tile_ID']\n",
    "            tim = row['datetime']\n",
    "            \n",
    "            if uid not in self.data:\n",
    "                self.data[uid] = [[pid, tim]]\n",
    "            else:\n",
    "                self.data[uid].append([pid, tim])\n",
    "                \n",
    "            if pid not in self.venues:\n",
    "                self.venues[pid] = 1\n",
    "            else:\n",
    "                self.venues[pid] += 1\n",
    "                \n",
    "                \n",
    "    def session_generation(self, df):\n",
    "\n",
    "        uid_3 = [x for x in self.data]\n",
    "        pick3 = sorted([(x, len(self.data[x])) for x in uid_3], key=lambda x: x[1], reverse=True)\n",
    "        pid_3 = [x for x in self.venues]\n",
    "        pid_pic3 = sorted([(x, self.venues[x]) for x in pid_3], key=lambda x: x[1], reverse=True)\n",
    "        pid_3 = dict(pid_pic3)\n",
    "        \n",
    "        session_len_list = []\n",
    "        \n",
    "        for u in pick3:\n",
    "            \n",
    "            uid = u[0]\n",
    "            info = self.data[uid]\n",
    "            topk = Counter([x[0] for x in info]).most_common()\n",
    "            topk1 = [x[0] for x in topk if x[1] > 1]\n",
    "            sessions = {}\n",
    "            \n",
    "            sub_df = df[df.user == str(uid)]\n",
    "            \n",
    "            for i, row in sub_df.iterrows():\n",
    "                sid = row['uid']\n",
    "                poi = row['tile_ID']\n",
    "                tmd = str(row['datetime'])\n",
    "                record = [poi, tmd]\n",
    "\n",
    "                if sid not in sessions:\n",
    "                    sessions[sid] = [record]\n",
    "                else:\n",
    "                    sessions[sid].append(record)\n",
    "            self.data_filter[uid] = {'sessions_count': len(sessions), 'topk_count': len(topk), 'topk': topk,\n",
    "                                         'sessions': sessions, 'raw_sessions': sessions}\n",
    "\n",
    "        self.user_filter3 = [x for x in self.data_filter]\n",
    "    \n",
    "    def build_users_locations_dict(self):\n",
    "        for u in self.user_filter3:\n",
    "            sessions = self.data_filter[u]['sessions']\n",
    "            if u not in self.uid_list:\n",
    "                self.uid_list[u] = [len(self.uid_list), len(sessions)]\n",
    "            for sid in sessions:\n",
    "                poi = [p[0] for p in sessions[sid]]\n",
    "                for p in poi:\n",
    "                    if p not in self.vid_list:\n",
    "                        self.vid_list_lookup[len(self.vid_list)] = p\n",
    "                        self.vid_list[p] = [len(self.vid_list), 1]\n",
    "                    else:\n",
    "                        self.vid_list[p][1] += 1\n",
    "                        \n",
    "     # support for radius of gyration\n",
    "    def load_venues(self,df,tess):\n",
    "        for i, row in df.iterrows():\n",
    "            \n",
    "            pid = row['tile_ID']\n",
    "            \n",
    "            if pid not in self.pid_loc_lat:\n",
    "            \n",
    "                lat = tess[tess.tile_ID == str(pid)]['geometry'].centroid.values[0].x\n",
    "                lon = tess[tess.tile_ID == str(pid)]['geometry'].centroid.values[0].y\n",
    "                try:\n",
    "                    self.pid_loc_lat[pid] = [float(lon), float(lat)]\n",
    "                except Exception as e:\n",
    "                    print('error:{}'.format(e))\n",
    "                    print(lon)\n",
    "                    print(lat)\n",
    "\n",
    "    def venues_lookup(self):\n",
    "        for vid in self.vid_list_lookup:\n",
    "            pid = self.vid_list_lookup[vid]\n",
    "            lon_lat = self.pid_loc_lat[pid]\n",
    "            self.vid_lookup[vid] = lon_lat\n",
    "          \n",
    "    @staticmethod    \n",
    "    def tid_list_48(tmd):\n",
    "        tm = time.strptime(tmd, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if tm.tm_wday in [0, 1, 2, 3, 4]:\n",
    "            tid = tm.tm_hour\n",
    "        else:\n",
    "            tid = tm.tm_hour + 24\n",
    "        return tid\n",
    "        \n",
    "    def prepare_neural_data(self):\n",
    "        for u in self.uid_list:\n",
    "            sessions = self.data_filter[u]['sessions']\n",
    "            sessions_tran = {}\n",
    "            sessions_id = []\n",
    "            for sid in sessions:\n",
    "                sessions_tran[sid] = [[self.vid_list[p[0]][0], self.tid_list_48(p[1])] for p in\n",
    "                                      sessions[sid]]\n",
    "                sessions_id.append(sid)\n",
    "                \n",
    "            split_id = int(np.floor(self.train_split * len(sessions_id)))\n",
    "            split_validation = int(np.floor(self.validation_split * len(sessions_id)))\n",
    "            \n",
    "            if split_validation == 0:\n",
    "                split_validation = 1\n",
    "            \n",
    "            split_validation = split_id + split_validation\n",
    "                \n",
    "            train_id = sessions_id[:split_id]\n",
    "            validation_id = sessions_id[split_id : split_validation]\n",
    "            test_id = sessions_id[split_validation:]\n",
    "            \n",
    "            pred_len = sum([len(sessions_tran[i]) - 1 for i in train_id])\n",
    "            valid_len = sum([len(sessions_tran[i]) - 1 for i in test_id])\n",
    "            train_loc = {}\n",
    "            for i in train_id:\n",
    "                for sess in sessions_tran[i]:\n",
    "                    if sess[0] in train_loc:\n",
    "                        train_loc[sess[0]] += 1\n",
    "                    else:\n",
    "                        train_loc[sess[0]] = 1\n",
    "            \n",
    "            self.data_neural[self.uid_list[u][0]] = {'sessions': sessions_tran, 'train': train_id, 'test': test_id,\n",
    "                                                     'pred_len': pred_len, 'valid_len': valid_len,\n",
    "                                                     'train_loc': train_loc, 'validation': validation_id}\n",
    "            \n",
    "    def save_variables(self):\n",
    "        foursquare_dataset = {'data_neural': self.data_neural, 'vid_list': self.vid_list, 'uid_list': self.uid_list, 'data_filter': self.data_filter,\n",
    "                              'vid_lookup': self.vid_lookup}\n",
    "        pickle.dump(foursquare_dataset, open('data/taxi_porto_new_gen.pk', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.load_trajectories(mapped_porto_tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.session_generation(mapped_porto_tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.build_users_locations_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/mobility_baselines/venv/lib/python3.6/site-packages/ipykernel_launcher.py:96: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "/home/luca/mobility_baselines/venv/lib/python3.6/site-packages/ipykernel_launcher.py:97: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.load_venues(mapped_porto_tdf, tess_porto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.venues_lookup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.prepare_neural_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.save_variables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_ids = dict()\n",
    "last_id = 0\n",
    "\n",
    "with open('data/original_data/taxi_sf/_cabs.txt', 'r') as f:\n",
    "    ids = f.readlines()\n",
    "    for idx in ids:\n",
    "        if idx.split('\"')[1] not in taxi_ids:\n",
    "            taxi_ids[idx.split('\"')[1]] = last_id\n",
    "            last_id += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/537 [00:00<?, ?it/s]/home/luca/mobility_baselines/venv/lib/python3.6/site-packages/ipykernel_launcher.py:12: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 537/537 [46:50<00:00,  5.23s/it]\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "\n",
    "current_traj_id = 0\n",
    "\n",
    "for filename in tqdm(glob('data/original_data/taxi_sf/*.txt')):\n",
    "    \n",
    "    pred = 0\n",
    "    \n",
    "    if filename == 'data/original_data/taxi_sf/_cabs.txt':\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(filename, header=None, sep='\\s', index_col = None)\n",
    "    df[3] = pd.to_datetime(df[3],unit='s')\n",
    "    \n",
    "    fname = taxi_ids[(filename.split('/')[-1]).split('.')[0][4:]]\n",
    "    df['uid'] = fname\n",
    "    \n",
    "    df['traj_id'] = 'na'\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row[2] == 1 and pred == 0:\n",
    "            current_traj_id += 1 \n",
    "            df.loc[i, 'traj_id'] = current_traj_id\n",
    "            pred = 1\n",
    "        elif row[2] == 1 and pred == 1:\n",
    "            df.loc[i, 'traj_id'] = current_traj_id\n",
    "        elif row[2] == 0 and pred == 1:\n",
    "            pred = 0\n",
    "    \n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['uid',3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/original_data/sf_with_traj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/original_data/sf_with_traj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>uid</th>\n",
       "      <th>traj_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4193158</td>\n",
       "      <td>37.74891</td>\n",
       "      <td>-122.39757</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-04 21:53:33</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4193157</td>\n",
       "      <td>37.74861</td>\n",
       "      <td>-122.39748</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-04 21:54:33</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4193156</td>\n",
       "      <td>37.74866</td>\n",
       "      <td>-122.39749</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-04 21:55:33</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4193155</td>\n",
       "      <td>37.74872</td>\n",
       "      <td>-122.39752</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-04 21:56:33</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4193154</td>\n",
       "      <td>37.74882</td>\n",
       "      <td>-122.39756</td>\n",
       "      <td>0</td>\n",
       "      <td>2008-06-04 21:57:33</td>\n",
       "      <td>0</td>\n",
       "      <td>na</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0          1  2                    3  uid traj_id\n",
       "0     4193158  37.74891 -122.39757  0  2008-06-04 21:53:33    0      na\n",
       "1     4193157  37.74861 -122.39748  0  2008-06-04 21:54:33    0      na\n",
       "2     4193156  37.74866 -122.39749  0  2008-06-04 21:55:33    0      na\n",
       "3     4193155  37.74872 -122.39752  0  2008-06-04 21:56:33    0      na\n",
       "4     4193154  37.74882 -122.39756  0  2008-06-04 21:57:33    0      na"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['2'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'uid':'user'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = skmob.TrajDataFrame(df, latitude='0', longitude='1', datetime='3', user_id='traj_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = tdf.sort_by_uid_and_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = compression.compress(tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv('data/original_data/sf_with_traj_compressed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.read_csv('data/original_data/sf_with_traj_compressed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = skmob.TrajDataFrame(tdf, latitude='0', longitude='1', datetime='3', user_id='traj_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_tdf = tilers.tiler.get(\"squared\", base_shape='San Francisco, California, USA', meters=tile_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_tdf = pd.read_csv('data/original_data/tex_SanFrancisco.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_tdf = gpd.GeoDataFrame(tess_tdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_points = gpd.GeoDataFrame(tdf, geometry=gpd.points_from_xy(tdf.lng, tdf.lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_points.set_geometry(col='geometry', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_tdf['geometry'] = tess_tdf['geometry'].apply(wkt.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_tdf.set_geometry(col='geometry', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_df = gpd.sjoin(tdf_points, tess_tdf, how=\"inner\", op='intersects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_tdf = skmob.TrajDataFrame(sf_df, latitude='lat', longitude='lng', datetime='datetime', user_id='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_tdf = sf_tdf.sort_by_uid_and_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_tdf.to_csv('data/original_data/taxi_sf_mapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_tdf = pd.read_csv('data/original_data/taxi_sf_mapped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_tdf.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cutted = list(sf_tdf.user.unique())[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_tdf = sf_tdf[sf_tdf.user.isin(user_cutted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator_SF(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        self.venues = {}\n",
    "        self.words_original = []\n",
    "        self.words_lens = []\n",
    "        self.dictionary = dict()\n",
    "        self.words_dict = None\n",
    "        self.data_filter = {}\n",
    "        self.user_filter3 = None\n",
    "        self.uid_list = {}\n",
    "        self.vid_list = {'unk': [0, -1]}\n",
    "        self.vid_list_lookup = {}\n",
    "        self.vid_lookup = {}\n",
    "        self.pid_loc_lat = {}\n",
    "        self.data_neural = {}\n",
    "        \n",
    "        self.train_split = 0.7\n",
    "        self.validation_split = 0.1\n",
    "    \n",
    "    def load_trajectories(self, df):\n",
    "        for i, row in df.iterrows():\n",
    "            uid = row['user']\n",
    "            pid = row['tile_ID']\n",
    "            tim = row['datetime']\n",
    "            \n",
    "            if uid not in self.data:\n",
    "                self.data[uid] = [[pid, tim]]\n",
    "            else:\n",
    "                self.data[uid].append([pid, tim])\n",
    "                \n",
    "            if pid not in self.venues:\n",
    "                self.venues[pid] = 1\n",
    "            else:\n",
    "                self.venues[pid] += 1\n",
    "                \n",
    "                \n",
    "    def session_generation(self, df):\n",
    "\n",
    "        uid_3 = [x for x in self.data]\n",
    "        pick3 = sorted([(x, len(self.data[x])) for x in uid_3], key=lambda x: x[1], reverse=True)\n",
    "        pid_3 = [x for x in self.venues]\n",
    "        pid_pic3 = sorted([(x, self.venues[x]) for x in pid_3], key=lambda x: x[1], reverse=True)\n",
    "        pid_3 = dict(pid_pic3)\n",
    "        \n",
    "        session_len_list = []\n",
    "        \n",
    "        for u in pick3:\n",
    "            uid = u[0]\n",
    "            info = self.data[uid]\n",
    "            topk = Counter([x[0] for x in info]).most_common()\n",
    "            topk1 = [x[0] for x in topk if x[1] > 1]\n",
    "            sessions = {}\n",
    "            \n",
    "            sub_df = df[df.user == uid]\n",
    "            \n",
    "            for i, row in sub_df.iterrows():\n",
    "                sid = row['uid']\n",
    "                poi = row['tile_ID']\n",
    "                tmd = str(row['datetime'])\n",
    "                record = [poi, tmd]\n",
    "\n",
    "                if sid not in sessions:\n",
    "                    sessions[sid] = [record]\n",
    "                else:\n",
    "                    sessions[sid].append(record)\n",
    "            self.data_filter[uid] = {'sessions_count': len(sessions), 'topk_count': len(topk), 'topk': topk,\n",
    "                                         'sessions': sessions, 'raw_sessions': sessions}\n",
    "\n",
    "        self.user_filter3 = [x for x in self.data_filter]\n",
    "    \n",
    "    def build_users_locations_dict(self):\n",
    "        for u in self.user_filter3:\n",
    "            sessions = self.data_filter[u]['sessions']\n",
    "            if u not in self.uid_list:\n",
    "                self.uid_list[u] = [len(self.uid_list), len(sessions)]\n",
    "            for sid in sessions:\n",
    "                poi = [p[0] for p in sessions[sid]]\n",
    "                for p in poi:\n",
    "                    if p not in self.vid_list:\n",
    "                        self.vid_list_lookup[len(self.vid_list)] = p\n",
    "                        self.vid_list[p] = [len(self.vid_list), 1]\n",
    "                    else:\n",
    "                        self.vid_list[p][1] += 1\n",
    "                        \n",
    "     # support for radius of gyration\n",
    "    def load_venues(self,df,tess):\n",
    "        for i, row in df.iterrows():\n",
    "            \n",
    "            pid = row['tile_ID']\n",
    "            \n",
    "            if pid not in self.pid_loc_lat:\n",
    "\n",
    "                lat = tess[tess.tile_ID == pid]['geometry'].centroid.values[0].x\n",
    "                lon = tess[tess.tile_ID == pid]['geometry'].centroid.values[0].y\n",
    "                try:\n",
    "                    self.pid_loc_lat[pid] = [float(lon), float(lat)]\n",
    "                except Exception as e:\n",
    "                    print('error:{}'.format(e))\n",
    "                    print(lon)\n",
    "                    print(lat)\n",
    "\n",
    "    def venues_lookup(self):\n",
    "        for vid in self.vid_list_lookup:\n",
    "            pid = self.vid_list_lookup[vid]\n",
    "            lon_lat = self.pid_loc_lat[pid]\n",
    "            self.vid_lookup[vid] = lon_lat\n",
    "          \n",
    "    @staticmethod    \n",
    "    def tid_list_48(tmd):\n",
    "        tm = time.strptime(tmd, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if tm.tm_wday in [0, 1, 2, 3, 4]:\n",
    "            tid = tm.tm_hour\n",
    "        else:\n",
    "            tid = tm.tm_hour + 24\n",
    "        return tid\n",
    "        \n",
    "    def prepare_neural_data(self):\n",
    "        for u in self.uid_list:\n",
    "            sessions = self.data_filter[u]['sessions']\n",
    "            sessions_tran = {}\n",
    "            sessions_id = []\n",
    "            for sid in sessions:\n",
    "                sessions_tran[sid] = [[self.vid_list[p[0]][0], self.tid_list_48(p[1])] for p in\n",
    "                                      sessions[sid]]\n",
    "                sessions_id.append(sid)\n",
    "                \n",
    "            split_id = int(np.floor(self.train_split * len(sessions_id)))\n",
    "            split_validation = int(np.floor(self.validation_split * len(sessions_id)))\n",
    "            \n",
    "            if split_validation == 0:\n",
    "                split_validation = 1\n",
    "            \n",
    "            split_validation = split_id + split_validation\n",
    "                \n",
    "            train_id = sessions_id[:split_id]\n",
    "            validation_id = sessions_id[split_id : split_validation]\n",
    "            test_id = sessions_id[split_validation:]\n",
    "            \n",
    "            pred_len = sum([len(sessions_tran[i]) - 1 for i in train_id])\n",
    "            valid_len = sum([len(sessions_tran[i]) - 1 for i in test_id])\n",
    "            train_loc = {}\n",
    "            for i in train_id:\n",
    "                for sess in sessions_tran[i]:\n",
    "                    if sess[0] in train_loc:\n",
    "                        train_loc[sess[0]] += 1\n",
    "                    else:\n",
    "                        train_loc[sess[0]] = 1\n",
    "            \n",
    "            self.data_neural[self.uid_list[u][0]] = {'sessions': sessions_tran, 'train': train_id, 'test': test_id,\n",
    "                                                     'pred_len': pred_len, 'valid_len': valid_len,\n",
    "                                                     'train_loc': train_loc, 'validation': validation_id}\n",
    "            \n",
    "    def save_variables(self):\n",
    "        foursquare_dataset = {'data_neural': self.data_neural, 'vid_list': self.vid_list, 'uid_list': self.uid_list, 'data_filter': self.data_filter,\n",
    "                              'vid_lookup': self.vid_lookup}\n",
    "        pickle.dump(foursquare_dataset, open('data/taxi_sf_new_gen.pk', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DataGenerator_SF()\n",
    "a.load_trajectories(sf_tdf)\n",
    "a.session_generation(sf_tdf)\n",
    "\n",
    "a.build_users_locations_dict()\n",
    "a.load_venues(sf_tdf, tess_tdf)\n",
    "a.venues_lookup()\n",
    "a.prepare_neural_data()\n",
    "a.save_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
